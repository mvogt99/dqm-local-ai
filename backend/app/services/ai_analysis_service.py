"""
AI Analysis Service
Generated by: RTX 5090 (architecture), RTX 3050 (implementation), Expert AI (enhancement)
Provides AI-powered data quality analysis and anomaly detection

V90: Added RCA feature parity with Expert AI
- analyze_failure(result_id) - Analyze specific DQ failure
- batch_analyze(result_ids) - Batch analyze failures
- get_analyses() - Retrieve stored analyses
"""
import logging
import json
import asyncpg
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict, field
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class Insight:
    """Data quality insight."""
    insight_id: str
    category: str
    description: str
    recommendation: str
    severity: str  # low, medium, high
    affected_columns: List[str]


@dataclass
class Anomaly:
    """Detected anomaly in data."""
    anomaly_id: str
    anomaly_type: str
    table_name: str
    column_name: Optional[str]
    details: str
    detected_at: datetime
    severity: str


@dataclass
class RootCauseAnalysis:
    """V90: Root cause analysis result - Feature parity with Expert AI.
    V88: Added dama_dimension for DAMA Data Quality framework alignment.
    """
    result_id: int
    suggested_cause: str
    confidence_score: float
    ai_model: str
    supporting_evidence: List[str] = field(default_factory=list)
    remediation_steps: List[str] = field(default_factory=list)
    dama_dimension: str = ""  # V88: DAMA dimension for the failing rule
    dama_description: str = ""  # V88: DAMA dimension description
    created_at: datetime = None

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.utcnow()


class AIAnalysisService:
    """
    AI-powered data quality analysis service.
    
    Generated by RTX 3050, enhanced by Expert AI.
    Uses statistical analysis to detect anomalies and generate insights.
    """

    def __init__(self, pool=None):
        """Initialize with database pool."""
        self._pool = pool

    async def analyze_profile(self, profile_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze profiling results and generate insights."""
        insights = []
        table_name = profile_data.get("table_name", "unknown")
        columns = profile_data.get("columns", [])
        row_count = profile_data.get("row_count", 0)

        for col in columns:
            col_name = col.get("column_name", "")
            # Support both null_percent (from profiling) and null_percentage (legacy)
            null_pct = col.get("null_percent", col.get("null_percentage", 0)) or 0
            # Calculate distinct percentage from unique_count if available
            distinct_pct = col.get("distinct_percentage", 0) or 0
            if distinct_pct == 0 and row_count > 0:
                unique_count = col.get("unique_count", 0) or 0
                distinct_pct = (unique_count / row_count * 100) if unique_count > 0 else 0

            if null_pct > 50:
                insights.append(Insight(
                    insight_id=f"null_{table_name}_{col_name}",
                    category="data_quality",
                    description=f"Column {col_name} has {null_pct:.1f}% null values",
                    recommendation="Consider adding NOT NULL constraint",
                    severity="high" if null_pct > 80 else "medium",
                    affected_columns=[col_name]
                ))

            if 0 < distinct_pct < 5 and row_count > 100:
                insights.append(Insight(
                    insight_id=f"enum_{table_name}_{col_name}",
                    category="optimization",
                    description=f"Column {col_name} has low cardinality ({distinct_pct:.1f}%)",
                    recommendation="Consider using ENUM type or lookup table",
                    severity="low",
                    affected_columns=[col_name]
                ))

        return {
            "table_name": table_name,
            "analysis_timestamp": datetime.utcnow().isoformat(),
            "insights": [asdict(i) for i in insights],
            "insight_count": len(insights)
        }

    async def detect_anomalies(self, table_stats: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect anomalies in table statistics."""
        anomalies = []
        now = datetime.utcnow()

        for table_name, stats in table_stats.items():
            row_count = stats.get("row_count", 0)
            if row_count > 10_000_000:
                anomalies.append(Anomaly(
                    anomaly_id=f"large_table_{table_name}",
                    anomaly_type="performance",
                    table_name=table_name,
                    column_name=None,
                    details=f"Table has {row_count:,} rows",
                    detected_at=now,
                    severity="medium"
                ))
            if row_count == 0:
                anomalies.append(Anomaly(
                    anomaly_id=f"empty_table_{table_name}",
                    anomaly_type="data_completeness",
                    table_name=table_name,
                    column_name=None,
                    details="Table has no data",
                    detected_at=now,
                    severity="medium"
                ))

        return [asdict(a) for a in anomalies]

    # =========================================================================
    # V90: RCA Feature Parity Methods
    # =========================================================================

    async def analyze_failure(self, result_id: int, dsn: str) -> RootCauseAnalysis:
        """
        V90: Analyze a specific DQ result failure.
        Feature parity with Expert AI.
        """
        conn = await asyncpg.connect(dsn)
        try:
            # Get the result details with rule info
            result = await conn.fetchrow('''
                SELECT r.id, r.rule_id, r.passed, r.failed_count, r.total_count,
                       r.failure_samples, dq.rule_name, dq.table_name, dq.column_name,
                       dq.rule_type
                FROM data_quality_results r
                JOIN data_quality_rules dq ON r.rule_id = dq.id
                WHERE r.id = $1
            ''', result_id)

            if not result:
                raise ValueError(f"Result {result_id} not found")

            # Generate analysis based on rule type and failure data
            rule_name = result['rule_name']
            table_name = result['table_name']
            column_name = result['column_name']
            rule_type = result['rule_type']
            failed_count = result['failed_count']
            total_count = result['total_count']
            fail_rate = round(failed_count / total_count * 100, 2) if total_count > 0 else 0

            # Parse failure samples
            samples = result['failure_samples']
            if isinstance(samples, str):
                try:
                    samples = json.loads(samples)
                except:
                    samples = []

            # Generate root cause based on rule type
            root_cause, evidence, remediation = self._generate_root_cause(
                rule_type, table_name, column_name, failed_count, total_count, samples
            )

            # V88: Get DAMA dimension for the rule type
            dama_dimension, dama_description = self._get_dama_dimension(rule_type)

            analysis = RootCauseAnalysis(
                result_id=result_id,
                suggested_cause=root_cause,
                confidence_score=0.75,  # Default confidence
                ai_model="local-analysis-v90",
                supporting_evidence=evidence,
                remediation_steps=remediation,
                dama_dimension=dama_dimension,
                dama_description=dama_description
            )

            # Store the analysis
            await self._store_analysis(conn, analysis)

            return analysis

        finally:
            await conn.close()

    def _generate_root_cause(
        self, rule_type: str, table: str, column: str,
        failed_count: int, total_count: int, samples: List
    ) -> tuple:
        """Generate root cause analysis based on rule type."""
        fail_rate = round(failed_count / total_count * 100, 2) if total_count > 0 else 0

        if rule_type in ('null_check', 'not_null'):
            cause = f"Column '{column}' in table '{table}' has {failed_count} NULL values ({fail_rate}% of records). This indicates missing data during ETL or incomplete source records."
            evidence = [
                f"Found {failed_count} records with NULL values",
                f"Affects {fail_rate}% of total {total_count} records"
            ]
            remediation = [
                "Review source data extraction process",
                "Add NOT NULL constraint with default value",
                "Implement data validation at ingestion"
            ]
        elif rule_type == 'unique_check':
            cause = f"Column '{column}' in table '{table}' has {failed_count} duplicate values. This suggests either a data merge issue or missing deduplication step."
            evidence = [
                f"Found {failed_count} duplicate records",
                "Duplicates may indicate merge conflicts or repeated ingestion"
            ]
            remediation = [
                "Add UNIQUE constraint to prevent future duplicates",
                "Implement deduplication in ETL pipeline",
                "Review data merge logic"
            ]
        elif rule_type == 'range_check':
            cause = f"Column '{column}' in table '{table}' has {failed_count} values outside expected range. This may indicate data entry errors or upstream calculation issues."
            evidence = [
                f"Found {failed_count} out-of-range values",
                f"Sample violations: {str(samples[:3])[:100]}"
            ]
            remediation = [
                "Add CHECK constraint for valid range",
                "Review upstream calculations",
                "Implement input validation"
            ]
        else:
            cause = f"Data quality rule '{rule_type}' failed on column '{column}' in table '{table}' with {failed_count} violations."
            evidence = [f"Found {failed_count} failing records"]
            remediation = ["Review the specific rule definition", "Analyze sample failures manually"]

        return cause, evidence, remediation

    def _get_dama_dimension(self, rule_type: str) -> tuple:
        """V88: Get DAMA dimension and description for a rule type."""
        # Import the mappings from data_quality_rules
        from app.services.data_quality_rules import RULE_TYPE_TO_DAMA, DAMA_DIMENSIONS

        rule_type_lower = rule_type.lower()
        dimension = RULE_TYPE_TO_DAMA.get(rule_type_lower, 'accuracy')
        description = DAMA_DIMENSIONS.get(dimension, 'General data quality dimension')

        return dimension, description

    async def _store_analysis(self, conn: asyncpg.Connection, analysis: RootCauseAnalysis):
        """Store analysis in database."""
        # Ensure table exists
        await conn.execute('''
            CREATE TABLE IF NOT EXISTS root_cause_analysis (
                id SERIAL PRIMARY KEY,
                result_id INTEGER NOT NULL,
                suggested_cause JSONB,
                confidence_score FLOAT,
                ai_model VARCHAR(100),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        await conn.execute('''
            INSERT INTO root_cause_analysis (result_id, suggested_cause, confidence_score, ai_model, created_at)
            VALUES ($1, $2, $3, $4, $5)
        ''',
            analysis.result_id,
            json.dumps({
                "cause": analysis.suggested_cause,
                "evidence": analysis.supporting_evidence,
                "remediation": analysis.remediation_steps,
                # V88: Include DAMA dimension in stored analysis
                "dama_dimension": analysis.dama_dimension,
                "dama_description": analysis.dama_description
            }),
            analysis.confidence_score,
            analysis.ai_model,
            analysis.created_at
        )

    async def batch_analyze(self, result_ids: List[int], dsn: str) -> List[RootCauseAnalysis]:
        """V90: Analyze multiple failures in batch."""
        analyses = []
        for result_id in result_ids:
            try:
                analysis = await self.analyze_failure(result_id, dsn)
                analyses.append(analysis)
            except Exception as e:
                logger.error(f"Failed to analyze result {result_id}: {e}")
        return analyses

    async def get_analyses(self, dsn: str, limit: int = 50) -> List[Dict[str, Any]]:
        """V90: Get stored analyses."""
        conn = await asyncpg.connect(dsn)
        try:
            # Check if table exists
            table_exists = await conn.fetchval('''
                SELECT EXISTS (
                    SELECT FROM information_schema.tables
                    WHERE table_name = 'root_cause_analysis'
                )
            ''')

            if not table_exists:
                return []

            results = await conn.fetch('''
                SELECT rca.id, rca.result_id, rca.suggested_cause, rca.confidence_score,
                       rca.ai_model, rca.created_at, dqr.rule_id,
                       COALESCE(dq.rule_name, 'Unknown') as rule_name
                FROM root_cause_analysis rca
                LEFT JOIN data_quality_results dqr ON rca.result_id = dqr.id
                LEFT JOIN data_quality_rules dq ON dqr.rule_id = dq.id
                ORDER BY rca.created_at DESC
                LIMIT $1
            ''', limit)

            return [
                {
                    "id": r['id'],
                    "result_id": r['result_id'],
                    "analysis": r['suggested_cause'] if isinstance(r['suggested_cause'], dict) else json.loads(r['suggested_cause'] or "{}"),
                    "confidence_score": float(r['confidence_score']) if r['confidence_score'] else 0,
                    "ai_model": r['ai_model'],
                    "created_at": r['created_at'].isoformat() if r['created_at'] else None,
                    "rule_name": r['rule_name']
                }
                for r in results
            ]
        finally:
            await conn.close()


_ai_analysis_service: Optional[AIAnalysisService] = None

async def get_ai_analysis_service(pool=None) -> AIAnalysisService:
    global _ai_analysis_service
    if _ai_analysis_service is None:
        _ai_analysis_service = AIAnalysisService(pool)
    return _ai_analysis_service


# Backward compatibility wrapper for api_routes.py
async def analyze_data_quality(profile_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Backward-compatible wrapper for AI analysis.

    Args:
        profile_data: Profiling results to analyze

    Returns:
        Analysis results with insights
    """
    service = await get_ai_analysis_service()
    return await service.analyze_profile(profile_data)
