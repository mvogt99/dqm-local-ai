"""
Data Quality Rules Service - Enhanced with Feature Parity
Generated by RTX 5090 (Qwen2.5-Coder-32B-Instruct-AWQ)
Enhanced by Expert AI (Claude Opus 4.5) - Added CRUD operations, custom rules, improved execution

V74: Feature parity with dqm-expert-ai
- Full CRUD operations for rules
- Custom SQL rule support
- Improved rule execution with detailed results
- Execution results persistence
"""
import asyncpg
import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)


# Table whitelist for SQL injection prevention
ALLOWED_TABLES = {
    'categories', 'suppliers', 'products', 'customers',
    'employees', 'shippers', 'orders', 'order_details'
}

# V87: DAMA Data Quality Dimensions
# Standard dimensions for categorizing data quality rules
DAMA_DIMENSIONS = {
    'accuracy': 'Data values accurately reflect real-world entities',
    'completeness': 'All required data values are present',
    'consistency': 'Data values are consistent across systems',
    'timeliness': 'Data is available when needed and up-to-date',
    'uniqueness': 'Duplicate records are minimized or eliminated',
    'validity': 'Data values conform to defined rules and constraints',
    'integrity': 'Relationships between data elements are correct',
    'precision': 'Data is at the right level of detail',
    'relevance': 'Data is appropriate for its intended use'
}

# Mapping of rule types to DAMA dimensions
RULE_TYPE_TO_DAMA = {
    'null_check': 'completeness',
    'not_null': 'completeness',
    'unique_check': 'uniqueness',
    'range_check': 'validity',
    'pattern_check': 'validity',
    'custom_sql': 'accuracy',
    'referential_check': 'integrity',
    'consistency_check': 'consistency',
    'freshness_check': 'timeliness'
}


@dataclass
class RuleResult:
    """Result of rule execution."""
    rule_id: int
    passed: bool
    total_count: int
    failed_count: int
    failure_samples: List[Dict[str, Any]] = field(default_factory=list)
    executed_at: datetime = None

    def __post_init__(self):
        if self.executed_at is None:
            self.executed_at = datetime.utcnow()

    @property
    def pass_rate(self) -> float:
        if self.total_count == 0:
            return 100.0
        return round((self.total_count - self.failed_count) / self.total_count * 100, 2)

    def to_dict(self) -> Dict[str, Any]:
        return {
            'rule_id': self.rule_id,
            'passed': self.passed,
            'total_count': self.total_count,
            'failed_count': self.failed_count,
            'pass_rate': self.pass_rate,
            'failure_samples': self.failure_samples,
            'executed_at': self.executed_at.isoformat() if self.executed_at else None
        }


class DataQualityRulesService:
    """Service for managing data quality rules with enhanced features."""

    def __init__(self, dsn: str):
        self.dsn = dsn

    def _validate_table(self, table: str) -> str:
        """Validate table name against whitelist to prevent SQL injection."""
        if table.lower() not in ALLOWED_TABLES:
            raise ValueError(f"Table '{table}' not in allowed list: {ALLOWED_TABLES}")
        return table.lower()

    async def _ensure_tables_exist(self, conn: asyncpg.Connection) -> None:
        """Ensure required tables exist in the database.

        V86: Updated to match existing database schema:
        - rule_name instead of name
        - rule_definition (JSONB) instead of definition (TEXT)
        """
        await conn.execute('''
            CREATE TABLE IF NOT EXISTS data_quality_rules (
                id SERIAL PRIMARY KEY,
                rule_name VARCHAR(255) NOT NULL,
                table_name VARCHAR(255) NOT NULL,
                column_name VARCHAR(255),
                rule_type VARCHAR(50) NOT NULL,
                rule_definition JSONB NOT NULL,
                severity VARCHAR(20) DEFAULT 'warning',
                is_active BOOLEAN DEFAULT true,
                description TEXT,
                threshold REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        await conn.execute('''
            CREATE TABLE IF NOT EXISTS data_quality_results (
                id SERIAL PRIMARY KEY,
                rule_id INTEGER REFERENCES data_quality_rules(id) ON DELETE CASCADE,
                passed BOOLEAN NOT NULL,
                total_count INTEGER DEFAULT 0,
                failed_count INTEGER DEFAULT 0,
                failure_samples JSONB,
                executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

    # =========================================================================
    # RULE RETRIEVAL
    # =========================================================================

    async def get_all_rules(self, active_only: bool = True) -> List[Dict]:
        """Get all data quality rules, optionally filtered by active status.

        V86: Updated to use correct column names (rule_name, rule_definition as JSONB).
        """
        conn = await asyncpg.connect(self.dsn)
        try:
            await self._ensure_tables_exist(conn)

            if active_only:
                query = '''
                    SELECT id, rule_name, table_name, column_name,
                           rule_type, rule_definition, severity, is_active, created_at
                    FROM data_quality_rules
                    WHERE is_active = true
                    ORDER BY created_at DESC
                '''
            else:
                query = '''
                    SELECT id, rule_name, table_name, column_name,
                           rule_type, rule_definition, severity, is_active, created_at
                    FROM data_quality_rules
                    ORDER BY created_at DESC
                '''

            rules = await conn.fetch(query)
            return [
                {
                    'id': r['id'],
                    'name': r['rule_name'],
                    'table': r['table_name'],
                    'column': r['column_name'],
                    'rule_type': r['rule_type'],
                    'definition': self._extract_definition(r['rule_definition']),
                    'severity': r['severity'],
                    'is_active': r['is_active'],
                    'created_at': r['created_at'].isoformat() if r['created_at'] else None,
                    # V87: Add DAMA dimension based on rule type
                    'dama_dimension': RULE_TYPE_TO_DAMA.get(r['rule_type'], 'accuracy'),
                    'dama_description': DAMA_DIMENSIONS.get(RULE_TYPE_TO_DAMA.get(r['rule_type'], 'accuracy'), '')
                }
                for r in rules
            ]
        except Exception as e:
            logger.error(f"Failed to get rules: {e}")
            return []
        finally:
            await conn.close()

    def _extract_definition(self, rule_definition) -> str:
        """Extract SQL definition from JSONB rule_definition.

        V86: rule_definition is stored as JSONB with structure like:
        {"sql": "SELECT ...", "threshold": 5} or just a string
        """
        if rule_definition is None:
            return ""
        if isinstance(rule_definition, str):
            return rule_definition
        if isinstance(rule_definition, dict):
            # Try common keys for SQL definition
            return rule_definition.get('sql') or rule_definition.get('query') or rule_definition.get('definition') or str(rule_definition)
        return str(rule_definition)

    async def get_rule_by_id(self, rule_id: int) -> Optional[Dict]:
        """Get a specific rule by ID.

        V86: Updated to use correct column names (rule_name, rule_definition).
        """
        conn = await asyncpg.connect(self.dsn)
        try:
            await self._ensure_tables_exist(conn)
            rule = await conn.fetchrow('''
                SELECT id, rule_name, table_name, column_name,
                       rule_type, rule_definition, severity, is_active, created_at
                FROM data_quality_rules
                WHERE id = $1
            ''', rule_id)

            if not rule:
                return None

            return {
                'id': rule['id'],
                'name': rule['rule_name'],
                'table': rule['table_name'],
                'column': rule['column_name'],
                'rule_type': rule['rule_type'],
                'definition': self._extract_definition(rule['rule_definition']),
                'severity': rule['severity'],
                'is_active': rule['is_active'],
                'description': rule['rule_definition'].get('description') if isinstance(rule['rule_definition'], dict) else None,
                'threshold': rule['rule_definition'].get('threshold') if isinstance(rule['rule_definition'], dict) else None,
                'created_at': rule['created_at'].isoformat() if rule['created_at'] else None
            }
        finally:
            await conn.close()

    # =========================================================================
    # RULE CREATION
    # =========================================================================

    async def create_rule(
        self,
        name: str,
        table: str,
        column: str,
        rule_type: str,
        definition: str,
        severity: str = 'warning'
    ) -> int:
        """Create a new data quality rule.

        V86: Updated to use correct column names (rule_name, rule_definition as JSONB).
        """
        # Validate table name
        self._validate_table(table)

        conn = await asyncpg.connect(self.dsn)
        try:
            await self._ensure_tables_exist(conn)
            # Store definition as JSONB
            rule_definition = json.dumps({'sql': definition, 'type': rule_type})
            result = await conn.fetchval('''
                INSERT INTO data_quality_rules
                (rule_name, table_name, column_name, rule_type, rule_definition, severity)
                VALUES ($1, $2, $3, $4, $5::jsonb, $6)
                RETURNING id
            ''', name, table, column, rule_type, rule_definition, severity)
            return result
        finally:
            await conn.close()

    async def create_custom_rule(
        self,
        name: str,
        table: str,
        column: str,
        custom_sql: str,
        description: Optional[str] = None,
        severity: str = 'warning',
        threshold: Optional[float] = None
    ) -> int:
        """Create a custom SQL-based rule.

        V86: Updated to use correct column names (rule_name, rule_definition as JSONB).
        """
        # Validate table name
        self._validate_table(table)

        conn = await asyncpg.connect(self.dsn)
        try:
            await self._ensure_tables_exist(conn)
            # Store definition as JSONB with additional metadata
            rule_definition = json.dumps({
                'sql': custom_sql,
                'type': 'custom_sql',
                'description': description,
                'threshold': threshold
            })
            result = await conn.fetchval('''
                INSERT INTO data_quality_rules
                (rule_name, table_name, column_name, rule_type, rule_definition, severity)
                VALUES ($1, $2, $3, 'custom_sql', $4::jsonb, $5)
                RETURNING id
            ''', name, table, column, rule_definition, severity)
            return result
        finally:
            await conn.close()

    # =========================================================================
    # RULE MODIFICATION
    # =========================================================================

    async def update_rule(self, rule_id: int, **kwargs) -> None:
        """Update a rule with the provided fields.

        V86: Updated to use correct column names (rule_name, rule_definition as JSONB).
        """
        if not kwargs:
            return

        conn = await asyncpg.connect(self.dsn)
        try:
            # Build dynamic update query
            set_clauses = []
            values = []
            param_num = 1

            # V86: Map API field names to database column names
            field_mapping = {
                'name': 'rule_name',
                'severity': 'severity',
                'is_active': 'is_active'
            }

            for key, value in kwargs.items():
                if key in field_mapping:
                    set_clauses.append(f"{field_mapping[key]} = ${param_num}")
                    values.append(value)
                    param_num += 1
                elif key == 'definition':
                    # Definition needs to be stored as JSONB
                    set_clauses.append(f"rule_definition = ${param_num}::jsonb")
                    values.append(json.dumps({'sql': value}))
                    param_num += 1

            if not set_clauses:
                return

            values.append(rule_id)
            query = f'''
                UPDATE data_quality_rules
                SET {', '.join(set_clauses)}
                WHERE id = ${param_num}
            '''

            await conn.execute(query, *values)
        finally:
            await conn.close()

    async def delete_rule(self, rule_id: int) -> None:
        """Delete a rule and its execution history."""
        conn = await asyncpg.connect(self.dsn)
        try:
            # Delete results first (cascade should handle this, but be explicit)
            await conn.execute(
                'DELETE FROM data_quality_results WHERE rule_id = $1',
                rule_id
            )
            await conn.execute(
                'DELETE FROM data_quality_rules WHERE id = $1',
                rule_id
            )
        finally:
            await conn.close()

    # =========================================================================
    # RULE SUGGESTION
    # =========================================================================

    async def suggest_rules(self, profiling_results: List[Dict]) -> List[Dict]:
        """
        Suggest data quality rules based on profiling results.

        Rule Types:
        - null_check: For columns with null values
        - unique_check: For columns that appear to be unique identifiers
        - range_check: For numeric columns with potential outliers
        - pattern_check: For string columns with consistent patterns
        - not_null: For critical columns that should never be null

        Enhanced by RTX 5090 to provide diverse, column-specific rules.
        """
        suggested_rules = []

        for result in profiling_results:
            table = result.get('table', 'unknown')
            column = result.get('column', 'unknown')
            null_pct = result.get('null_percentage', 0) or 0
            unique_count = result.get('unique_count', 0) or 0
            data_type = result.get('data_type', '').lower()
            min_value = result.get('min_value')
            max_value = result.get('max_value')

            # Rule 1: NULL Check - For columns with nulls (DAMA: Completeness)
            if null_pct > 0.01:  # More than 1% nulls
                severity = 'critical' if null_pct > 0.1 else 'warning' if null_pct > 0.05 else 'info'
                suggested_rules.append({
                    'name': f"null_check_{table}_{column}",
                    'table': table,
                    'column': column,
                    'rule_type': 'null_check',
                    'definition': f"SELECT * FROM {table} WHERE {column} IS NULL",
                    'severity': severity,
                    'reason': f"Column has {null_pct*100:.1f}% null values",
                    'threshold': 0 if null_pct < 0.05 else 5,
                    'confidence': round(1.0 - null_pct, 2),
                    # V87: DAMA DQ Dimension
                    'dama_dimension': 'completeness',
                    'dama_description': DAMA_DIMENSIONS['completeness']
                })

            # Rule 2: NOT NULL Check - For columns that SHOULD be null-free (DAMA: Completeness)
            if null_pct == 0 and column.lower() in ['id', 'name', 'email', 'customer_id', 'order_id', 'product_id']:
                suggested_rules.append({
                    'name': f"not_null_{table}_{column}",
                    'table': table,
                    'column': column,
                    'rule_type': 'not_null',
                    'definition': f"SELECT * FROM {table} WHERE {column} IS NULL",
                    'severity': 'critical',
                    'reason': f"Critical column should never be null",
                    'confidence': 1.0,
                    # V87: DAMA DQ Dimension
                    'dama_dimension': 'completeness',
                    'dama_description': DAMA_DIMENSIONS['completeness']
                })

            # Rule 3: Unique Check - For columns that appear to be unique (DAMA: Uniqueness)
            if unique_count > 0 and column.lower().endswith(('_id', 'id', 'email', 'phone', 'code')):
                suggested_rules.append({
                    'name': f"unique_check_{table}_{column}",
                    'table': table,
                    'column': column,
                    'rule_type': 'unique_check',
                    'definition': f"SELECT {column}, COUNT(*) FROM {table} GROUP BY {column} HAVING COUNT(*) > 1",
                    'severity': 'warning',
                    'reason': f"Column appears to be a unique identifier",
                    'confidence': 0.85,
                    # V87: DAMA DQ Dimension
                    'dama_dimension': 'uniqueness',
                    'dama_description': DAMA_DIMENSIONS['uniqueness']
                })

            # Rule 4: Range Check - For numeric columns (DAMA: Validity)
            if data_type in ['integer', 'numeric', 'decimal', 'float', 'double', 'int', 'bigint', 'smallint', 'real']:
                if column.lower() in ['price', 'unit_price', 'amount', 'total', 'cost', 'quantity', 'units_in_stock', 'units_on_order', 'reorder_level']:
                    suggested_rules.append({
                        'name': f"range_check_{table}_{column}",
                        'table': table,
                        'column': column,
                        'rule_type': 'range_check',
                        'definition': f"SELECT * FROM {table} WHERE {column} < 0",
                        'severity': 'critical',
                        'reason': f"Numeric column should have non-negative values",
                        'min_value': 0,
                        'confidence': 0.9,
                        # V87: DAMA DQ Dimension
                        'dama_dimension': 'validity',
                        'dama_description': DAMA_DIMENSIONS['validity']
                    })

                # Add range boundary check if min/max available (DAMA: Accuracy)
                if min_value is not None and max_value is not None:
                    suggested_rules.append({
                        'name': f"boundary_check_{table}_{column}",
                        'table': table,
                        'column': column,
                        'rule_type': 'range_check',
                        'definition': f"SELECT * FROM {table} WHERE {column} < {min_value} OR {column} > {max_value}",
                        'severity': 'warning',
                        'reason': f"Values outside observed range [{min_value}, {max_value}]",
                        'min_value': min_value,
                        'max_value': max_value,
                        'confidence': 0.7,
                        # V87: DAMA DQ Dimension
                        'dama_dimension': 'accuracy',
                        'dama_description': DAMA_DIMENSIONS['accuracy']
                    })

            # Rule 5: Pattern Check - For string columns with expected formats (DAMA: Validity)
            if data_type in ['varchar', 'text', 'character varying', 'char']:
                if 'email' in column.lower():
                    suggested_rules.append({
                        'name': f"pattern_check_{table}_{column}",
                        'table': table,
                        'column': column,
                        'rule_type': 'pattern_check',
                        'definition': f"SELECT * FROM {table} WHERE {column} IS NOT NULL AND {column} !~ '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{{2,}}$'",
                        'severity': 'warning',
                        'reason': f"Email column should match email format",
                        'pattern': '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$',
                        'confidence': 0.85,
                        # V87: DAMA DQ Dimension
                        'dama_dimension': 'validity',
                        'dama_description': DAMA_DIMENSIONS['validity']
                    })
                elif 'phone' in column.lower() or 'fax' in column.lower():
                    suggested_rules.append({
                        'name': f"pattern_check_{table}_{column}",
                        'table': table,
                        'column': column,
                        'rule_type': 'pattern_check',
                        'definition': f"SELECT * FROM {table} WHERE {column} IS NOT NULL AND {column} !~ '^[0-9()\\-+ .]+$'",
                        'severity': 'info',
                        'reason': f"Phone column should contain valid phone characters",
                        'pattern': '^[0-9()\\-+ .]+$',
                        'confidence': 0.75,
                        # V87: DAMA DQ Dimension
                        'dama_dimension': 'validity',
                        'dama_description': DAMA_DIMENSIONS['validity']
                    })
                elif 'postal' in column.lower() or 'zip' in column.lower():
                    suggested_rules.append({
                        'name': f"pattern_check_{table}_{column}",
                        'table': table,
                        'column': column,
                        'rule_type': 'pattern_check',
                        'definition': f"SELECT * FROM {table} WHERE {column} IS NOT NULL AND {column} !~ '^[A-Za-z0-9 -]+$'",
                        'severity': 'info',
                        'reason': f"Postal code should match expected format",
                        'pattern': '^[A-Za-z0-9 -]+$',
                        'confidence': 0.7,
                        # V87: DAMA DQ Dimension
                        'dama_dimension': 'validity',
                        'dama_description': DAMA_DIMENSIONS['validity']
                    })

        return suggested_rules

    # =========================================================================
    # RULE EXECUTION
    # =========================================================================

    async def execute_rule(self, rule_id: int) -> Dict[str, Any]:
        """Execute a data quality rule and return detailed results.

        V86: Updated to use correct column names (rule_name, rule_definition).
        """
        conn = await asyncpg.connect(self.dsn)
        try:
            await self._ensure_tables_exist(conn)

            # Get rule details
            rule = await conn.fetchrow(
                'SELECT * FROM data_quality_rules WHERE id = $1',
                rule_id
            )
            if not rule:
                raise ValueError(f"Rule {rule_id} not found")

            table = rule['table_name']
            column = rule['column_name']
            rule_type = rule['rule_type']
            # V86: Extract definition from JSONB rule_definition
            rule_definition = rule['rule_definition']
            definition = self._extract_definition(rule_definition)
            # Extract threshold from JSONB or use default
            threshold = 0
            if isinstance(rule_definition, dict):
                threshold = rule_definition.get('threshold', 0) or 0

            # Execute based on rule type
            if rule_type == 'custom_sql':
                result = await self._execute_custom_sql(conn, table, definition, threshold)
            elif rule_type == 'null_check' or rule_type == 'not_null':
                result = await self._execute_null_check(conn, table, column, threshold)
            elif rule_type == 'unique_check':
                result = await self._execute_unique_check(conn, table, column)
            elif rule_type == 'range_check':
                result = await self._execute_range_check(conn, table, column, definition)
            elif rule_type == 'pattern_check':
                result = await self._execute_pattern_check(conn, table, column, definition)
            else:
                # Default: execute the definition as SQL and count failures
                result = await self._execute_generic(conn, table, definition)

            result.rule_id = rule_id

            # Store result
            await self._store_result(conn, result)

            return result.to_dict()
        finally:
            await conn.close()

    async def _execute_null_check(
        self, conn: asyncpg.Connection, table: str, column: str, threshold: float
    ) -> RuleResult:
        """Execute null check rule."""
        # Get total count
        total = await conn.fetchval(f'SELECT COUNT(*) FROM "{table}"')

        # Get null count
        null_count = await conn.fetchval(
            f'SELECT COUNT(*) FROM "{table}" WHERE "{column}" IS NULL'
        )

        null_pct = (null_count / total * 100) if total > 0 else 0
        passed = null_pct <= threshold

        # Get sample failures
        samples = []
        if null_count > 0:
            sample_rows = await conn.fetch(
                f'SELECT * FROM "{table}" WHERE "{column}" IS NULL LIMIT 5'
            )
            samples = [dict(row) for row in sample_rows]

        return RuleResult(
            rule_id=0,
            passed=passed,
            total_count=total,
            failed_count=null_count,
            failure_samples=samples
        )

    async def _execute_unique_check(
        self, conn: asyncpg.Connection, table: str, column: str
    ) -> RuleResult:
        """Execute uniqueness check rule."""
        # Find duplicates
        duplicates = await conn.fetch(f'''
            SELECT "{column}", COUNT(*) as cnt
            FROM "{table}"
            GROUP BY "{column}"
            HAVING COUNT(*) > 1
        ''')

        total = await conn.fetchval(f'SELECT COUNT(*) FROM "{table}"')
        failed = sum(row['cnt'] - 1 for row in duplicates)

        samples = [
            {'value': str(row[column]), 'count': row['cnt']}
            for row in duplicates[:5]
        ]

        return RuleResult(
            rule_id=0,
            passed=len(duplicates) == 0,
            total_count=total,
            failed_count=failed,
            failure_samples=samples
        )

    async def _execute_range_check(
        self, conn: asyncpg.Connection, table: str, column: str, definition: str
    ) -> RuleResult:
        """Execute range check rule."""
        # Execute the definition SQL to find violations
        failures = await conn.fetch(definition)
        total = await conn.fetchval(f'SELECT COUNT(*) FROM "{table}"')

        samples = [dict(row) for row in failures[:5]]

        return RuleResult(
            rule_id=0,
            passed=len(failures) == 0,
            total_count=total,
            failed_count=len(failures),
            failure_samples=samples
        )

    async def _execute_pattern_check(
        self, conn: asyncpg.Connection, table: str, column: str, definition: str
    ) -> RuleResult:
        """Execute pattern check rule."""
        # Execute the definition SQL to find violations
        failures = await conn.fetch(definition)
        total = await conn.fetchval(
            f'SELECT COUNT(*) FROM "{table}" WHERE "{column}" IS NOT NULL'
        )

        samples = [dict(row) for row in failures[:5]]

        return RuleResult(
            rule_id=0,
            passed=len(failures) == 0,
            total_count=total,
            failed_count=len(failures),
            failure_samples=samples
        )

    async def _execute_custom_sql(
        self, conn: asyncpg.Connection, table: str, custom_sql: str, threshold: float
    ) -> RuleResult:
        """Execute custom SQL rule."""
        failures = await conn.fetch(custom_sql)
        total = await conn.fetchval(f'SELECT COUNT(*) FROM "{table}"')

        fail_pct = (len(failures) / total * 100) if total > 0 else 0
        passed = fail_pct <= threshold

        samples = [dict(row) for row in failures[:5]]

        return RuleResult(
            rule_id=0,
            passed=passed,
            total_count=total,
            failed_count=len(failures),
            failure_samples=samples
        )

    async def _execute_generic(
        self, conn: asyncpg.Connection, table: str, definition: str
    ) -> RuleResult:
        """Execute generic SQL definition."""
        failures = await conn.fetch(definition)
        total = await conn.fetchval(f'SELECT COUNT(*) FROM "{table}"')

        samples = [dict(row) for row in failures[:5]]

        return RuleResult(
            rule_id=0,
            passed=len(failures) == 0,
            total_count=total,
            failed_count=len(failures),
            failure_samples=samples
        )

    async def _store_result(self, conn: asyncpg.Connection, result: RuleResult) -> int:
        """Store rule execution result."""
        result_id = await conn.fetchval('''
            INSERT INTO data_quality_results
            (rule_id, passed, total_count, failed_count, failure_samples, executed_at)
            VALUES ($1, $2, $3, $4, $5, $6)
            RETURNING id
        ''',
            result.rule_id,
            result.passed,
            result.total_count,
            result.failed_count,
            json.dumps(result.failure_samples, default=str),
            result.executed_at
        )
        return result_id

    # =========================================================================
    # EXECUTION RESULTS
    # =========================================================================

    async def get_execution_results(
        self, rule_id: Optional[int] = None, limit: int = 100
    ) -> List[Dict]:
        """Get execution results, optionally filtered by rule.

        V86: Updated to use correct column name (rule_name instead of name).
        """
        conn = await asyncpg.connect(self.dsn)
        try:
            await self._ensure_tables_exist(conn)

            if rule_id:
                results = await conn.fetch('''
                    SELECT r.id, r.rule_id, dq.rule_name, r.passed,
                           r.total_count, r.failed_count, r.failure_samples, r.executed_at
                    FROM data_quality_results r
                    JOIN data_quality_rules dq ON r.rule_id = dq.id
                    WHERE r.rule_id = $1
                    ORDER BY r.executed_at DESC
                    LIMIT $2
                ''', rule_id, limit)
            else:
                results = await conn.fetch('''
                    SELECT r.id, r.rule_id, dq.rule_name, r.passed,
                           r.total_count, r.failed_count, r.failure_samples, r.executed_at
                    FROM data_quality_results r
                    JOIN data_quality_rules dq ON r.rule_id = dq.id
                    ORDER BY r.executed_at DESC
                    LIMIT $1
                ''', limit)

            return [
                {
                    'id': r['id'],
                    'rule_id': r['rule_id'],
                    'rule_name': r['rule_name'],
                    'passed': r['passed'],
                    'total_count': r['total_count'],
                    'failed_count': r['failed_count'],
                    'pass_rate': round(
                        (r['total_count'] - r['failed_count']) / r['total_count'] * 100, 2
                    ) if r['total_count'] > 0 else 100.0,
                    'failure_samples': r['failure_samples'],
                    'executed_at': r['executed_at'].isoformat() if r['executed_at'] else None
                }
                for r in results
            ]
        except Exception as e:
            logger.error(f"Failed to get execution results: {e}")
            return []
        finally:
            await conn.close()

    async def get_failures(self, rule_id: int) -> List[str]:
        """Get failures for a specific rule (legacy method)."""
        results = await self.get_execution_results(rule_id=rule_id, limit=10)
        return [str(r.get('failure_samples', [])) for r in results]
