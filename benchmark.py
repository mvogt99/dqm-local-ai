"""
Performance Benchmark: LOCAL AI vs Expert AI DQM Applications
Generated by RTX 5090 (Qwen2.5-Coder-32B-AWQ)
Compares response times across key endpoints
"""
import asyncio
import argparse
import json
import statistics
from datetime import datetime
from typing import Dict, List, Any
import httpx


class BenchmarkResults:
    """Store and analyze benchmark results."""

    def __init__(self, latencies: List[float]):
        self.latencies = sorted(latencies)
        self.count = len(latencies)

    @property
    def min_ms(self) -> float:
        return min(self.latencies) if self.latencies else 0

    @property
    def max_ms(self) -> float:
        return max(self.latencies) if self.latencies else 0

    @property
    def avg_ms(self) -> float:
        return statistics.mean(self.latencies) if self.latencies else 0

    @property
    def p50_ms(self) -> float:
        return statistics.median(self.latencies) if self.latencies else 0

    @property
    def p95_ms(self) -> float:
        if not self.latencies:
            return 0
        idx = int(self.count * 0.95)
        return self.latencies[min(idx, self.count - 1)]

    @property
    def p99_ms(self) -> float:
        if not self.latencies:
            return 0
        idx = int(self.count * 0.99)
        return self.latencies[min(idx, self.count - 1)]

    def to_dict(self) -> Dict[str, float]:
        return {
            "min_ms": round(self.min_ms, 2),
            "max_ms": round(self.max_ms, 2),
            "avg_ms": round(self.avg_ms, 2),
            "p50_ms": round(self.p50_ms, 2),
            "p95_ms": round(self.p95_ms, 2),
            "p99_ms": round(self.p99_ms, 2),
            "count": self.count
        }


async def fetch(client: httpx.AsyncClient, endpoint: str) -> float:
    """Make request and return latency in milliseconds."""
    start = datetime.now()
    try:
        response = await client.get(endpoint, timeout=30.0)
        response.raise_for_status()
    except Exception as e:
        print(f"  Error: {endpoint} - {e}")
        return -1
    return (datetime.now() - start).total_seconds() * 1000


async def benchmark_endpoint(
    base_url: str,
    endpoint: str,
    iterations: int,
    warmup: int = 10
) -> BenchmarkResults:
    """Benchmark a single endpoint with warmup and measurement phases."""
    async with httpx.AsyncClient(base_url=base_url) as client:
        # Warmup phase
        print(f"  Warming up {endpoint} ({warmup} requests)...")
        for _ in range(warmup):
            await fetch(client, endpoint)

        # Measurement phase
        print(f"  Measuring {endpoint} ({iterations} requests)...")
        latencies = []
        for i in range(iterations):
            latency = await fetch(client, endpoint)
            if latency >= 0:
                latencies.append(latency)
            if (i + 1) % 25 == 0:
                print(f"    Progress: {i + 1}/{iterations}")

    return BenchmarkResults(latencies)


async def run_benchmarks(iterations: int) -> Dict[str, Any]:
    """Run all benchmarks and return results."""
    apps = {
        "local_ai": {
            "base_url": "http://localhost:8001",
            "endpoints": {
                "health": "/health",
                "list_tables": "/api/profile/tables",
                "profile_customers": "/api/profile/customers"
            }
        },
        "expert_ai": {
            "base_url": "http://localhost:8002",
            "endpoints": {
                "health": "/health",
                "list_tables": "/data-profiling/tables",
                "profile_customers": "/data-profiling/tables/customers/columns"
            }
        }
    }

    results = {}

    for app_name, config in apps.items():
        print(f"\n{'='*60}")
        print(f"Benchmarking: {app_name.upper()}")
        print(f"Base URL: {config['base_url']}")
        print(f"{'='*60}")

        results[app_name] = {}

        for endpoint_name, endpoint_path in config["endpoints"].items():
            print(f"\nEndpoint: {endpoint_name}")
            try:
                bench_result = await benchmark_endpoint(
                    config["base_url"],
                    endpoint_path,
                    iterations,
                    warmup=min(10, iterations // 10)
                )
                results[app_name][endpoint_name] = bench_result.to_dict()
                print(f"  Avg: {bench_result.avg_ms:.2f}ms, P95: {bench_result.p95_ms:.2f}ms")
            except Exception as e:
                print(f"  FAILED: {e}")
                results[app_name][endpoint_name] = {"error": str(e)}

    return results


def print_comparison_table(results: Dict[str, Any]):
    """Print formatted comparison table."""
    print("\n" + "=" * 100)
    print("PERFORMANCE COMPARISON: LOCAL AI vs Expert AI")
    print("=" * 100)

    header = f"{'Endpoint':<25} | {'App':<12} | {'Min':>8} | {'Avg':>8} | {'P50':>8} | {'P95':>8} | {'P99':>8} | {'Max':>8}"
    print(header)
    print("-" * 100)

    endpoints = set()
    for app_results in results.values():
        endpoints.update(app_results.keys())

    for endpoint in sorted(endpoints):
        for app_name, app_results in results.items():
            if endpoint in app_results and "error" not in app_results[endpoint]:
                data = app_results[endpoint]
                row = f"{endpoint:<25} | {app_name:<12} | {data['min_ms']:>7.1f} | {data['avg_ms']:>7.1f} | {data['p50_ms']:>7.1f} | {data['p95_ms']:>7.1f} | {data['p99_ms']:>7.1f} | {data['max_ms']:>7.1f}"
                print(row)
        print("-" * 100)

    # Winner analysis
    print("\nWINNER ANALYSIS (lower is better):")
    for endpoint in sorted(endpoints):
        local = results.get("local_ai", {}).get(endpoint, {})
        expert = results.get("expert_ai", {}).get(endpoint, {})

        if "avg_ms" in local and "avg_ms" in expert:
            local_avg = local["avg_ms"]
            expert_avg = expert["avg_ms"]
            diff = local_avg - expert_avg
            winner = "Expert AI" if diff > 0 else "LOCAL AI"
            diff_pct = abs(diff) / max(local_avg, expert_avg) * 100
            print(f"  {endpoint}: {winner} wins by {abs(diff):.2f}ms ({diff_pct:.1f}%)")


async def main():
    parser = argparse.ArgumentParser(
        description="Benchmark LOCAL AI vs Expert AI DQM applications"
    )
    parser.add_argument(
        "--iterations", "-n",
        type=int,
        default=100,
        help="Number of iterations per endpoint (default: 100)"
    )
    parser.add_argument(
        "--output", "-o",
        type=str,
        default="benchmark_results.json",
        help="Output JSON file (default: benchmark_results.json)"
    )
    args = parser.parse_args()

    print(f"DQM Performance Benchmark")
    print(f"Iterations: {args.iterations}")
    print(f"Output: {args.output}")

    # Run benchmarks
    results = await run_benchmarks(args.iterations)

    # Add metadata
    output = {
        "metadata": {
            "timestamp": datetime.now().isoformat(),
            "iterations": args.iterations,
            "generated_by": "RTX 5090 (Qwen2.5-Coder-32B-AWQ)"
        },
        "results": results
    }

    # Save to file
    with open(args.output, "w") as f:
        json.dump(output, f, indent=2)
    print(f"\nResults saved to: {args.output}")

    # Print comparison
    print_comparison_table(results)


if __name__ == "__main__":
    asyncio.run(main())
